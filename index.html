<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>RL Mahjong Agent</title>
  <link rel="stylesheet" href="style.css">
  <script>
    function toggleMenu() {
      const menu = document.getElementById("side-menu");
      const content = document.getElementById("main-content");
      if (menu.classList.contains("open")) {
        menu.classList.remove("open");
        content.classList.remove("shifted");
      } else {
        menu.classList.add("open");
        content.classList.add("shifted");
      }
    }
  </script>
  <script>
    src="https://polyfill.io/v3/polyfill.min.js?features=es6">
  </script>
  <script
          id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>


  <style>
#side-menu {
  position: fixed;
  top: 0;
  left: 0;
  width: 250px;
  height: 100vh;
  overflow-y: auto;
  background-color: #f5f5f5;
  padding: 20px;
  box-shadow: 2px 0 5px rgba(0, 0, 0, 0.1);
  z-index: 1000;
  transition: transform 0.3s ease;
  transform: translateX(-100%);
}

#side-menu.open {
  transform: translateX(0);
}

#main-content {
  margin-left: 0;
  transition: margin-left 0.3s ease;
  padding: 0 5%;
}

#main-content.shifted {
  margin-left: 250px;
}


/* Default: no padding */
  #main-content {
    padding: 0 5%;
  }

  /* Apply 20% side padding only to these sections */
  #results,
  #methodology {
    padding: 0 15%;
    text-align: left;
    margin: 40px 0;
  }

  /* Keep text left-aligned by default */
  #main-content h2,
  #main-content p,
  #main-content ul,
  #main-content table {
    text-align: left;
  }

  /* Only center the demo section */
  #demo.centered-section {
    padding: 0 15%;
    text-align: left;
    margin: 40px 0;
  }

  #demo video {

    max-width: 100%;
  }
</style>

</head>
<body>



<div id="page">

<div id="side-menu">
  <div class="menu-title">Menu</div>
  <a href="#rlmahjong">Abstract</a>
  <a href="#mahjong-intro">Introduction to Mahjong</a>

  <div class="submenu">
    <a href="#tiles">Tiles</a>
    <a href="#rules">Rules</a>
    <a href="#yaku">Yaku</a>
    <a href="#glossary">Glossary</a>
  </div>

  <a href="#methodology">Methodology</a>
  <div class="submenu">
    <a href="#pipeline-architecture">Model Pipeline and Architecture</a>
    <a href="#tools-and-dataset">Tools and Datasets</a>
    <a href="#dataset">Tenhou Dataset</a>
    <a href="#mjx">MJX Framework</a>
    <a href="#rewards">Reward System</a>
  </div>

  <a href="#results">Experiments & Results</a>
  <div class="submenu">
    <a href="#supervised-performance">Supervised Learning</a>
    <a href="#initial-comparisons">Initial Comparisons</a>
    <a href="#final-performance">Final Performance</a>
  </div>

  <a href="#demo">Demo</a>
  <a href="#conclusion">Conclusion</a>
  <a href="#citations">Citations</a>
</div>

  <div id="main-content">
    <div style="width: 100%; display: flex; flex-direction: column; align-items: center; margin-top:
    30px; margin-bottom: 40px;">
  <h1 style="font-size: 3.5em; margin-bottom: 10px; text-align: center;">MahjongMind: Reinforcement
  Learning in Imperfect Information, Multi-Agent, Competitve Environments</h1>
  <p style="font-size: 1.5em; margin: 0; text-align: center;">Jason Zhang, Amy Kim, Eric Lin</p>
</div>
    <button id="menu-button" onclick="toggleMenu()">☰</button>
<section id="rlmahjong" class="section-with-image">
  <div class="text">
    <h2>Abstract</h2>
    <p> Riichi Mahjong is a fast-paced, four-player game that involves hidden information,
      long-term planning, and strategic decision-making. Because of its complexity and uncertainty,
      it’s a great environment for testing AI agents. </p>
    <p> In this project, we introduce <strong>MahjongMind</strong>, a reinforcement
      learning agent trained using self-play in the MJX simulation environment.
      The agent uses a mix of learned behavior from PPO and simple rule-based logic to make
      smarter, more stable decisions. </p>
    <p> We tested MahjongMind against several other
    agents including (most notably) AlphaJong, a strong search-based AI that has reached Master rank on Mahjong websites.
      Over 1000 games, MahjongMind came close to matching AlphaJong’s performance,
      winning 32.9% of games and gaining an average of +20.07 in rating.
      This was done with only 200,000 training steps, showing that even with limited compute,
      our approach can be competitive with much more complex systems. </p>
  </div>
  <div class="image">
    <img src="images/tenhou_gameplay.jpg" alt="RL Mahjong Intro Image">
  </div>
</section>

    <section id="mahjong-intro">
      <h2>Introduction to Mahjong</h2>

      <div class="mahjong-grid">
        <!-- Row 1: Tiles and Rules -->
        <div class="mahjong-subsection" id="tiles">
          <div class="section-image">
            <img src="images/mahjong-tiles.png" alt="Tiles Example">
          </div>
          <h3>Tiles</h3>
          <p>In Riichi Mahjong, tiles are divided into several categories: suits and
            honors. Suited tiles include characters (manzu), bamboo (souzu), and
            circles (pinzu), each numbered from one to nine. Honor tiles consist
            of winds (east, south, west, north) and dragons (white, green, and red).
            Each tile type appears four times, resulting in a total of 136 tiles used
            in the game. Understanding the distribution and relationships between these
            tile groups is critical for identifying efficient paths to complete a hand,
            assessing discards, and predicting opponents' hands based on observed behavior.</p>
          <!-- Button -->
          <a href="https://en.wikipedia.org/wiki/Mahjong_tiles" class="info-button"
             target="_blank">More Info</a>
        </div>

        <div class="mahjong-subsection" id="rules">
          <div class="section-image">
            <img src="images/mahjong-rules.png" alt="Rules Example">
          </div>
          <h3>Rules</h3>
          <p>Riichi Mahjong is a four-player game centered around assembling
            a legal winning hand of fourteen tiles. A complete hand typically
            consists of four sets (three tiles forming sequences or triplets)
            and one pair, although exceptions exist for special hands. Players
            draw and discard tiles turn-by-turn, aiming to complete their hand before
            others. A unique feature of Riichi Mahjong is the concept of "riichi,"
            a declaration made when a player is one tile away from winning, locking
            in their hand structure and increasing potential points. Rules also
            govern calls (pon, chi, kan) that allow stealing tiles from others,
            and scoring is influenced by both hand structure and hidden bonuses
            such as dora.</p>
          <!-- Button -->
          <a href="https://mahjong.guide/a-beginners-guide-to-riichi-mahjong/"
             class="info-button" target="_blank">More Info</a>
        </div>

        <!-- Row 2: Yaku and Glossary -->
  <div class="mahjong-subsection" id="yaku">
  <h3>Yaku</h3>
  <p>In Riichi Mahjong, a hand must satisfy at least one "yaku", a specific pattern or condition, to be
    eligible for winning. Below are some basic yaku important for both players and
    AI agents to recognize:</p>

  <ul>

    <li>
      <strong>Riichi:</strong> Declare ready hand; must have a closed hand.
      <div class="tile-row">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
      </div>
    </li>

    <li>
      <strong>Tanyao (All Simples):</strong> Hand composed entirely of 2–8 numbered tiles.
      <div class="tile-row">
        <img src="images/mahjong_tiles_png/Mpt2m.png" alt="2-man">
        <img src="images/mahjong_tiles_png/Mpt3m.png" alt="3-man">
        <img src="images/mahjong_tiles_png/Mpt4m.png" alt="4-man">
        <img src="images/mahjong_tiles_png/Mpt5p.png" alt="5-pin">
        <img src="images/mahjong_tiles_png/Mpt6p.png" alt="6-pin">
        <img src="images/mahjong_tiles_png/Mpt7p.png" alt="7-pin">
        <img src="images/mahjong_tiles_png/Mpt5s.png" alt="5-sou">
        <img src="images/mahjong_tiles_png/Mpt6s.png" alt="6-sou">
        <img src="images/mahjong_tiles_png/Mpt7s.png" alt="7-sou">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
      </div>
    </li>

    <li>
      <strong>Pinfu:</strong> Hand made entirely of sequences, no points for waits.
      <div class="tile-row">
        <img src="images/mahjong_tiles_png/Mpt2m.png" alt="2-man">
        <img src="images/mahjong_tiles_png/Mpt3m.png" alt="3-man">
        <img src="images/mahjong_tiles_png/Mpt4m.png" alt="4-man">
        <img src="images/mahjong_tiles_png/Mpt6p.png" alt="6-pin">
        <img src="images/mahjong_tiles_png/Mpt7p.png" alt="7-pin">
        <img src="images/mahjong_tiles_png/Mpt8p.png" alt="8-pin">
        <img src="images/mahjong_tiles_png/Mpt3s.png" alt="3-sou">
        <img src="images/mahjong_tiles_png/Mpt4s.png" alt="4-sou">
        <img src="images/mahjong_tiles_png/Mpt5s.png" alt="5-sou">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
      </div>
    </li>

    <li>
      <strong>Yakuhai (Value Tiles):</strong> Triplet of dragons or seat winds.
      <div class="tile-row">
        <img src="images/mahjong_tiles_png/Mpt5q.png" alt="Red Dragon">
        <img src="images/mahjong_tiles_png/Mpt5q.png" alt="Red Dragon">
        <img src="images/mahjong_tiles_png/Mpt5q.png" alt="Red Dragon">
        <img src="images/mahjong_tiles_png/Mpt7p.png" alt="7-pin">
        <img src="images/mahjong_tiles_png/Mpt8p.png" alt="8-pin">
        <img src="images/mahjong_tiles_png/Mpt9p.png" alt="9-pin">
        <img src="images/mahjong_tiles_png/Mpt2s.png" alt="2-sou">
        <img src="images/mahjong_tiles_png/Mpt3s.png" alt="3-sou">
        <img src="images/mahjong_tiles_png/Mpt4s.png" alt="4-sou">
        <img src="images/mahjong_tiles_png/Mpt6m.png" alt="6-man">
        <img src="images/mahjong_tiles_png/Mpt7m.png" alt="7-man">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
      </div>
    </li>

    <li>
      <strong>Iipeikou (Pure Double Sequence):</strong> Two identical sequences in the same suit.
      <div class="tile-row">
        <img src="images/mahjong_tiles_png/Mpt3s.png" alt="3-sou">
        <img src="images/mahjong_tiles_png/Mpt4s.png" alt="4-sou">
        <img src="images/mahjong_tiles_png/Mpt5s.png" alt="5-sou">
        <img src="images/mahjong_tiles_png/Mpt3s.png" alt="3-sou">
        <img src="images/mahjong_tiles_png/Mpt4s.png" alt="4-sou">
        <img src="images/mahjong_tiles_png/Mpt5s.png" alt="5-sou">
        <img src="images/mahjong_tiles_png/Mpt2m.png" alt="2-man">
        <img src="images/mahjong_tiles_png/Mpt3m.png" alt="3-man">
        <img src="images/mahjong_tiles_png/Mpt4m.png" alt="4-man">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Hidden tile">
      </div>
    </li>

    <li>
      <strong>Menzen Tsumo:</strong> Winning by self-draw with a closed hand.
      <div class="tile-row">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
        <img src="images/mahjong_tiles_png/Mpt00.png" alt="Closed tile">
      </div>
    </li>
  </ul>
    <!-- Button -->
  <a href="https://riichi.wiki/List_of_yaku" class="info-button" target="_blank">More Info</a>

  </div>


        <div class="mahjong-subsection" id="glossary">
          <h3>Glossary</h3>
          <p>A collection of key terms frequently used in Riichi Mahjong
            gameplay, essential for understanding game flow and rules:</p>
          <ul>
            <li><strong>Riichi:</strong> Declaration of a ready hand by betting 1000 points.</li>
            <li><strong>Dora:</strong> Bonus tiles that increase hand value.</li>
            <li><strong>Tenpai:</strong> A hand one tile away from winning.</li>
            <li><strong>Furiten:</strong> Cannot win off discard if you have discarded a winning tile.</li>
            <li><strong>Tsumo:</strong> Winning by drawing the needed tile yourself.</li>
            <li><strong>Ron:</strong> Winning by claiming another player's discarded tile.</li>
            <li><strong>Yaku:</strong> Specific hand patterns or conditions required to win.</li>
            <li><strong>Chombo:</strong> Penalty for an illegal move, such as a false win.</li>
            <li><strong>Kan:</strong> Declaring a quad (four identical tiles) for extra draw and dora.</li>
            <li><strong>Chi:</strong> Claiming a sequential meld from the player to your left.</li>
            <li><strong>Pon:</strong> Claiming a triplet from any player's discard.</li>
            <li><strong>Closed Hand:</strong> A hand without any open melds (chi, pon, kan).</li>
            <li><strong>Open Hand:</strong> A hand with one or more open melds (revealed to others).</li>
            <li><strong>Dealer (East):</strong> The player who starts the round;
              receives extra rewards for winning but more penalty for losing.</li>
          </ul>
          <!-- Button -->
          <a href="https://riichi.wiki/List_of_terminology_by_alphabetical_order"
             class="info-button" target="_blank">More Info</a>
        </div>
      </div>
</section>



    <section id="methodology" class="centered-section">
  <h2>Methodology</h2>
  <h3 id="pipeline-architecture">Model Pipeline and Architecture</h3>

  <div class="image-container">
    <img src="images/mahjongmind_training_pipeline.png" alt="MahjongMind Training Pipeline" width="800">
    <img src="images/model_architecture.png" alt="Model Architecture" width="800">
    <img src="images/model_descriptions.png" alt="Model Descriptions" width="800">
  </div>

  <p>
The MahjongMind training pipeline consists of five main stages: Tenhou Game Logs, supervised learning, reinforcement learning, the self-play loop, and the final policy network. This structured approach enables the agent to first imitate expert players and then refine its strategies through self-play.
</p>

<ol>
  <li>
    <strong>Tenhou Game Logs</strong>: Historical gameplay data is collected from Tenhou, a popular online Japanese Mahjong platform. These logs contain sequences of game states and expert actions, serving as valuable training data for imitation learning.
  </li>
  <li>
    <strong>Supervised Learning</strong>: A neural network is trained using the Tenhou game logs to predict the most likely expert move given a game state. This supervised learning step allows the agent to mimic expert strategies and provides a strong initial policy.
  </li>
  <li>
    <strong>Reinforcement Learning</strong>: The pretrained policy network is further optimized using reinforcement learning. The agent engages in self-play—playing games against copies of itself—to explore new strategies and maximize long-term rewards. A reinforcement learning algorithm, such as REINFORCE or Proximal Policy Optimization (PPO), is used for this stage.
  </li>
  <li>
    <strong>Self-play Loop</strong>: Through repeated self-play, the agent continually refines its policy based on game outcomes, improving its performance beyond imitation. This helps it generalize to new situations and achieve higher levels of strategic reasoning.
  </li>
  <li>
    <strong>Final Policy Network</strong>: The outcome of this pipeline is a fully trained policy network capable of making high-quality decisions in real-time Mahjong play. It can be evaluated against human players or other agents for benchmarking.
  </li>
</ol>
      <h3 id="decision-schema">Decision Schema</h3>

<p>
Our Mahjong AI agent follows a hierarchical decision schema inspired by the structure of the
  MJX environment and strategies introduced in <a href="https://arxiv.org/abs/2003.13590"
       target="_blank">
    Suphx</a>, another paper focused on RL applications in Mahjong. The decision process respects the
  rules and priorities of Riichi Mahjong and is organized as follows:
</p>

<ul>
  <li>
    <strong>Action Type Filtering</strong>: At every game step, the
    MJX environment provides a list of legal actions. These are grouped by action type
    (e.g., discard, riichi, pon, chi, kan).
  </li>
  <li>
    <strong>Binary Head Decisions</strong>: If an action type has exactly
    two legal options (e.g., pon or pass), the corresponding model head
    (e.g., <code>pon_model</code>) outputs a softmax over the logits to decide.
  </li>
  <li>
    <strong>Discard Head (Multiclass)</strong>: If discard or tsumogiri is legal,
    the 34-way discard head is used to choose the best tile to discard.
  </li>
  <li>
    <strong>Other Legal Actions</strong>: Ron, Tsumo, and other actions are handled
    through reinforcement learning alone (i.e., no supervised component). In rare
    edge cases where no trained head applies, the agent defaults to selecting a legal action at random.
  </li>
</ul>

<p>
This design allows the agent to reuse supervised learning heads trained on Tenhou
  data while improving the shared representation through reinforcement learning with PPO.
</p>

      <h3 id="tools-and-dataset">Tools and Datasets</h3>

  <h4 id="dataset">Tenhou 4-Player Riichi Mahjong Dataset</h4>
  <p>
    We utilized the
    <a href="https://www.kaggle.com/datasets/hphphp123321/tenhou-4-player-riichi-mahjong-dataset/data"
       target="_blank">
    Tenhou 4-Player Riichi Mahjong Dataset</a>, which contains 51GB of gamelogs
    collected from online matches.
    Each record encodes full game logs including tile draws, discards, calls, and
    winning hands. These logs were preprocessed
    into training tuples representing decision points such as discards,
    riichi declarations, and calls (chi, pon, kan).
  </p>

  <h4 id="mjx">MJX Framework</h4>
  <p>
    For simulation and reinforcement learning, we used <a href="https://github.com/mjx-project/mjx"
                                                          target="_blank">MJX</a>,
    a fast and extensible Mahjong environment written in C++ and wrapped in Python.
    It provides full observability controls,
    legal action extraction, and support for self-play training with reinforcement
    learning algorithms. We leveraged
    MJX's ability to extract game states in a structured format to align with the
    supervised data and to implement
    agent evaluation pipelines.
  </p>
      <h3 id="rewards">Reward System</h3>
  <p>
    The reward system used during training is modeled after the <em>elo</em> system
    from <a href="https://tenhou.net" target="_blank">Tenhou.net</a>, the most widely
    used online platform for Riichi Mahjong. At the end of each game, elo is distributed as follows:
  </p>
  <ul>
    <li><strong>1st Rank</strong> (highest score): +90 elo</li>
    <li><strong>2nd place</strong>: +45 elo</li>
    <li><strong>3rd place</strong>: +0 elo</li>
    <li><strong>4th place</strong> (lowest score): –135 elo</li>
  </ul>
  <p>
    This system rewards defensive play over reckless decisions, encouraging agents to avoid
    risky discards rather than exploit weak opponents aggressively. It reflects the
    priorities of real-world competitive platforms, where avoiding losses is often more
    critical than maximizing short-term gains.
  </p>
  <p>
    While this design aligns well with the strategic objectives of
    Riichi Mahjong, alternative reward schemes, such as using raw score, could be developed for different
    goals (such as maximizing raw point gain), which would encourage more aggressive,
    exploitative behavior against weaker opponents.
  </p>
      <h3 id="rewards-training">Rewards During Training</h3>
  <p>
    During training, the agent receives a reward after each action:
  </p>
  <pre><code>obs, reward, done, info = env.step(action)</code></pre>

  <p>
    The reward value at each time step is accumulated over the course of an episode:
  </p>
  <p>
    \( \text{total_reward} = \sum_{t=0}^{T} r_t \)
  </p>
  <p>
    where \( r_t \) is the reward at time step \( t \), and \( T \) is the total number
    of time steps in the episode.
  </p>

  <h3>Reward Calculation</h3>
  <p>
    The agent's goal is to maximize its expected cumulative reward. This is done by
    optimizing the policy using methods such as Proximal Policy Optimization (PPO).
    The cumulative return \( R \) at time step \( t \) is calculated as:
  </p>
  <p>
    \( R_t = r_t + \gamma \cdot R_{t+1} \)
  </p>
  <ul>
    <li>\( r_t \): the immediate reward at time step \( t \)</li>
    <li>\( \gamma \): the discount factor, set to \( 0.99 \)</li>
    <li>\( R_{t+1} \): the return at the next time step</li>
  </ul>
</section>

<section id="results" class="centered-section">
  <h2>Experiments & Results</h2>

  <div id="supervised-performance" class="results-subsection">
    <h3>Supervised Learning Performance</h3>
    <p>
      We first trained our initial policy network using supervised learning on Tenhou game logs.
      This allowed the agent to mimic expert play, learning common discard choices and safe tile behaviors.
      The network was evaluated based on move prediction accuracy.
    </p>
<h4>Supervised Model Accuracy</h4>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Dataset Size</th>
      <th>Top-1</th>
      <th>Top-3</th>
      <th>Top-5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Discard</strong></td>
      <td>17GB</td>
      <td>0.7142</td>
      <td>0.8882</td>
      <td>0.9542</td>
    </tr>
    <tr>
      <td><strong>Riichi</strong></td>
      <td>6GB</td>
      <td>0.8263</td>
      <td>~</td>
      <td>~</td>
    </tr>
    <tr>
      <td><strong>Pon</strong></td>
      <td>12GB</td>
      <td>0.8722</td>
      <td>~</td>
      <td>~</td>
    </tr>
    <tr>
      <td><strong>Chi</strong></td>
      <td>12GB</td>
      <td>0.8524</td>
      <td>~</td>
      <td>~</td>
    </tr>
    <tr>
      <td><strong>Kan</strong></td>
      <td>5GB</td>
      <td>0.9486</td>
      <td>~</td>
      <td>~</td>
    </tr>
  </tbody>
</table>
<p>
  These results highlight the utility of imitation learning in modeling structured decision-making in Mahjong.
  The discard model, trained on the largest dataset segment (17GB), achieves a top-5 accuracy of 95.42%,
  indicating reliable alignment with expert discard behavior. Models for binary decision tasks (Riichi, Pon,
  Chi, and Kan) also show high top-1 accuracy despite smaller dataset sizes, with the Kan model
  reaching 94.86% using only 5GB. The lack of top-3 and top-5 metrics for these tasks reflects
  their binary nature. Overall, the supervised models effectively capture expert priors and provide
  a strong initialization for subsequent reinforcement learning.
</p>

  </div>

  <div id="initial-comparisons" class="results-subsection">
    <h3>Initial Comparisons</h3>
    <p>
      To evaluate the effectiveness of reinforcement learning without supervised initialization,
      we trained agents using PPO and REINFORCE from scratch over 200,000 self-play games in addition to the
      model trained using expert data.
      These agents were benchmarked against a random baseline, where agents select uniformly
      from legal actions at each decision point. This setup tests whether reinforcement learning
      alone—absent any expert priors—can yield competitive policies. Each trained agent was then
      evaluated over 100 simulated games against the random policy, providing a measure of relative
      performance purely attributable to the learning algorithm and interaction dynamics.
    </p>

    <h4>Average Score by Agent Type (Over 100 Game Simulation)</h4>

<table>
  <thead>
    <tr>
      <th>Agent Type</th>
      <th>Avg Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>PPO</td>
      <td>+29.3</td>
    </tr>
    <tr>
      <td>Supervised Learning</td>
      <td>+18.9</td>
    </tr>
    <tr>
      <td>REINFORCE</td>
      <td>–8.5</td>
    </tr>
    <tr>
      <td>Random (Baseline)</td>
      <td>–39.7</td>
    </tr>
  </tbody>
</table>
    <p>
      The results indicate that PPO is the most effective of the evaluated approaches,
      achieving the highest average score (+29.3) after training from scratch. This suggests
      that policy optimization with stable gradient updates can discover competent strategies in
      Mahjong without supervised pretraining. In contrast, REINFORCE underperforms (–8.5), likely
      due to its high variance and slower convergence in multi-agent, sparse-reward settings. The
      supervised learning agent performs competitively (+18.9), showing that expert imitation alone
      can capture much of the structure of play, though it lacks the adaptability of reinforcement
      learning. As expected, the random agent performs poorly (–39.7), reinforcing the need for policy
      learning. These comparisons demonstrate that while reinforcement learning is effective on its own,
      combining it with prior knowledge offers further advantages.


    </p>
  </div>

  <div id="final-performance" class="results-subsection">
    <h3>Final Performance</h3>

    <p>
      To test the final performance of our final agent (trained using supervised learning to initialize model
      weights and PPO self-play to further improve policy network), we play 1000 rounds of Riichi Mahjong
      against 3 other agents, described below:
    </p>

    <div class="agent-description">
      <h4>RuleBasedAgent</h4>
      <p>
        A deterministic agent using hard-coded expert logic, designed as a strong rule-based baseline.
        It mimics human play using structured if-else decision logic and prioritizes winning moves (Tsumo/Ron),
        Riichi declarations, and safe/strategic calls. Effective for benchmarking basic
        competence without learning.
      </p>
    </div>

    <div class="agent-description">
      <h4>ShantenAgent</h4>
      <p>
        A hand-efficiency-focused agent that plays solely to reduce the <em>shanten</em>
        number (the distance from a winning hand).
        Inspired by Mjai's ShantenPlayer, it prioritizes draws/discards
        and melds that optimize efficiency, without regard to risk or opponent play.
        Represents a greedy but goal-driven heuristic.
      </p>
    </div>

    <div class="agent-description">
      <h4>AlphaJong</h4>
      <p>
        A sophisticated, search-based agent inspired by AlphaGo’s philosophy. Instead of using
        machine learning,
        <a href="https://github.com/Jimboom7/AlphaJong" target="_blank">AlphaJong</a> simulates
        future game states to evaluate possible actions and selects the most promising move.
        It effectively balances offense and defense through structured turn simulations.
        <strong>Notably, AlphaJong has reached <em>Master rank</em></strong>,
        one of the highest competitive tiers in Mahjong Soul’s ranked ladder, indicating strong
        performance against human players.
      </p>
    </div>

    <p>
      This table summarizes the placement distribution, average game score, and rating change
      for each agent over 1000 games.
    </p>

    <table>
  <caption>Performance of Agents in Simulated Mahjong Games (1000 rounds)</caption>
  <thead>
    <tr>
      <th>Agent ID</th>
      <th>1st Rank</th>
      <th>2nd Rank</th>
      <th>3rd Rank</th>
      <th>4th Rank</th>
      <th>Avg. Score</th>
      <th>Avg. Tenhou Rating Gain/Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>AlphaJong</td>
      <td>38.3%</td>
      <td>28.4%</td>
      <td>17.6%</td>
      <td>15.7%</td>
      <td>29,304.60</td>
      <td>+25.97</td>
    </tr>
    <tr>
      <td>ShantenAgent</td>
      <td>15.2%</td>
      <td>19.6%</td>
      <td>31.8%</td>
      <td>33.4%</td>
      <td>21,027.00</td>
      <td>-22.73</td>
    </tr>
    <tr>
      <td>RuleBasedAgent</td>
      <td>13.6%</td>
      <td>23.2%</td>
      <td>29.2%</td>
      <td>34.0%</td>
      <td>21,341.50</td>
      <td>-23.31</td>
    </tr>
    <tr>
      <td>MahjongMind</td>
      <td>32.9%</td>
      <td>28.8%</td>
      <td>21.4%</td>
      <td>16.9%</td>
      <td>28,011.90</td>
      <td>+20.07</td>
    </tr>
  </tbody>
</table>
    <p> <strong>MahjongMind</strong> demonstrates highly competitive performance,
      closely trailing the master-level AlphaJong in all metrics. With a 32.9% 1st place rate and a low 16.9%
      4th place rate, MahjongMind shows both offensive strength and defensive reliability.
      Its average score of 28,011.90 and a Tenhou rating gain of +20.07 affirm that it is capable of
      consistently high-level play, even outperforming AlphaJong in terms of middle of the table performances. </p>

      <p>Notably, these results were achieved after only 200,000 PPO training iterations,
      a relatively small training budget constrained by compute and time limitations.
      With more training samples and hyperparameter refinement, MahjongMind is well-positioned to
      surpass AlphaJong in overall performance.</p>

      <p> In contrast to static rule-based and Shanten-focused agents, which suffer from high
        last-place rates and negative rating drift, MahjongMind strikes a strong balance
        between risk and reward. Its hybrid architecture with reinforcement learning guided by
        structural heuristics appears to yield more adaptable and effective decision-making,
        setting a promising foundation for future improvements. </p>
  </div>

</section>

<section id="demo" class="centered-section">
  <h2>Demo</h2>
  <p>
    Below is a sample match played by the agents in the final results table.
    In the replay, the perspective is fixed with our agent at the bottom, AlphaJong on the left,
    the ShantenAgent on the top, and the RuleBasedAgent on the right.
  </p>

  <video id="demo-video" width="850" controls>
  <source src="videos/replay.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</section>

<section id="conclusion" style="padding: 0 15%; text-align: left; margin: 40px 0;">
  <h2>Conclusion</h2>
<p> <strong>MahjongMind</strong> demonstrates that a hybrid architecture with reinforcement learning guided
  by rule-based structure can produce strong performance in complex, imperfect information environments.
  Despite being trained under significant computational constraints, the agent closely matched the
  performance of AlphaJong, a search-based system tuned for expert-level play. </p>
  <p> Achieving a competitive win rate, low last-place finishes, and substantial rating gains,
    MahjongMind outperformed all non-search baselines and remained within reach of AlphaJong across
    all metrics. These results underscore the effectiveness of structured learning over
    purely handcrafted or search-intensive approaches. </p>
  <p> The findings suggest that with increased training and refinement, such hybrid
    models can scale beyond current performance benchmarks. This direction offers
    a promising foundation for future work in multi-agent strategy, particularly in
    high-variance domains where stability and risk aversion are essential. </p>
</section>
  </div>
</div> <!-- Close #page -->
</body>
</html>